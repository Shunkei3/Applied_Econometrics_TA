---
title: "Review Assignment 3"
author: "Shunkei Kakimoto, University of Nebraska Lincoln"
output:
  xaringan::moon_reader:
    # css: [default, metropolis, metropolis-fonts] 
    css: xaringan-themer_large.css 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: middle

```{r, child = '../setup.Rmd'}
```

```{r, include = F}
opts_knit$set(root.dir = "~/Dropbox/R-project/Applied_Econometrics_TA")

#--- load packages ---#
# suppressMessages(library(here))

# /*---- Data Wrangling ----*/
library(data.table)
library(tidyverse)

# --- regression --- #
# library(here)
library(fixest)
library(readstata13)
library(readr)

# /*---- Visualization ----*/
library(RColorBrewer)
library(patchwork)
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(viridis)
library(grid)
library(gridExtra)
library(GGally)

# /*---- Model Summary ----*/
library(stats)
library(modelsummary)
library(flextable)
library(officer)
library(officedown)
library(gt)
```

```{r, echo=F}
library(readstata13)
library(modelsummary)
library(car)
library(fixest)
library(tidyverse)
library(broom)
library(here)
```

# Today's Main Topics

## Various Econometric Modeling
+ Various functional form
  * Logarithmic function
  * Quadratic term
  * Interaction term
  * Dummy variables 

<br>

+ The consequence of functional form misspecification
  * Omitted variable bias

<br>

## Standard Error Estimation
+ Heteroskedasticity and Clustered Error
+ Consequences of heteroskedasticity and cluster error for the property of OLS estimator


---

## Problems
+ Problem 3
  * Topics: How to interpret a model including interactions of Dummy variables
  * Sub-topic: Model using interactions involving dummy variables v.s. Model using separate dummy variables 

<br>

+ Problem 6 
  * Topics: Direction of bias (A quick review)

<br>

+ Problem C1 
  * Topics: The importance of functional forms
  * Sub-topic: More on logarithmic functional forms

<br>

+ Problem C13
  * Topics: Standard Error Estimation
  
<br>

+ 4 Data management
  * Topics: How to merge two data that have different data format


---

# 2.1 Problem 3 (Chapter 7) **Two category cases: gender and race**

<span style="color:blue">Using the data in GPA2.RAW, the following equation was estimated:</span>

$$
sat = \beta_0 + \beta_1 hsize + \beta_2 hsize^2 +\beta_3 female +  \beta_4 black + \beta_5 female \times black
$$

.left5[

<br>

+ `sat` : the combined SAT score <br/>
+ `hsize`: size of the student's high school graduating class <br/>

<br>

.content-box-blue[**Dummy variables**]
+ `female` : a gender dummy variable
+ `black` : a race dummy variable

<br>

By using these dummy variables, we want to understand **how the SAT scores different among nonblack-male, nonblack-female, black-male, and black-female students, on average**. 
]



.right5[

<br>

<span style="color:blue">The below is the regression results</span>

```{r 3_7_not_remove, echo=FALSE}
library(data.table)
gpa_data <- read.dta13("./Data/Assignment_3/gpa2.dta") %>%
  data.table() %>%
  .[, hsize_2 := hsize^2] %>%
  .[, black_female := female * black]

reg_gpa <- feols(sat ~ hsize + hsize_2 + female + black + I(female*black), data = gpa_data)

msummary(
  reg_gpa,
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```
]

---

## Interpretation of the coefficients of dummy variables (1)

$$
\hat{sat} = \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 +\hat{\beta}_3 female +  \hat{\beta}_4 black + \hat{\beta}_5 female \times black 
$$

<!-- \begin{align}
\hat{sat} &= \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 +\hat{\beta}_3 female +  \hat{\beta}_4 black + \hat{\beta}_5 female \times black 
\\
\hat{sat} &= 1028.097 + 19.297 hsize −2.195 hsize^2 −45.091 female −169.813 black + 62.306 female \times black
\end{align} -->

<br>

.content-box-blue[**Step1:**]: What is the base group? 
+ The base group is the case when `black=0` and `female=0`, which is nonblack-male students. 
  * So, the interpretation of an individual coefficient on the dummy variable is how much more (less) students belonging to that category are expected to get SAT score compared to nonblack-male students holding everything else fixed. 

<br>

+ For nonblack-male students: `black=0` and `female=0`,

$$
\hat{sat} = \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2
$$

$\hat{\beta}_0$ indicates the average SAT score of nonblack-male students.


---

## Interpretation of the coefficients of dummy variables (2)

$$
\hat{sat} = \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 +\hat{\beta}_3 female +  \hat{\beta}_4 black + \hat{\beta}_5 female \times black 
$$

<br>

.content-box-green[**Step1: What is the base group? **]: 
+ The base group is the case when `black=0` and `female=0`, which is nonblack-male students. 

<br>

.content-box-green[**Step2: How do we know the coefficients of the other groups?**]

+ You can simply plug in the correct combination of zeros and ones to `black` and `female` by each category.

**For nonblack-female students: `black=0` and `female=1`, so**

\begin{align}
\hat{sat} 
&= \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 + \color{blue}{\hat{\beta}_3} 
\\
&= \hat{\beta}_0 + \color{blue}{\hat{\beta}_3} + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 
\end{align}


**For black-male students: `black=1` and `female=0`, so**

\begin{align}
\hat{sat} &= \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 +  \color{blue}{\hat{\beta}_4} \\
&= \hat{\beta}_0 + \color{blue}{\hat{\beta}_4}  + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2
\end{align}


**For black-female students: `black=1` and `female=1`, so**

\begin{align}
\hat{sat} &= \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 + \color{blue}{\hat{\beta}_3 +  \hat{\beta}_4 + \hat{\beta}_5} \\
&= \hat{\beta}_0 + (\color{blue}{\hat{\beta}_3 +  \hat{\beta}_4 + \hat{\beta}_5}) + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2
\end{align}

<br>

+ As you can see, the coefficients $\color{blue}{\hat{\beta}_3}$, $\color{blue}{\hat{\beta}_4}$, $\color{blue}{\hat{\beta}_5}$ are additional information about how much different from $\hat{\beta_0}$ (the average SAT score of nonblack-male students)

---

<span style="color:blue">(ii) Holding $hsize$ fixed, what is the estimated difference in SAT score between **nonblack-females** and **nonblack-males**? How statistically significant is this estimated difference?</span>

<br>

**Nonblack-female students: (`black=0` and `female=1`) v.s.  Nonblack-males students: (`black=0` and `female=0`: base group)**

For Nonblack-female students:

<br>

$$
\hat{sat} = \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 + \color{blue}{\hat{\beta}_3} 
$$

<br>

+ Hence, the estimated difference is $\color{blue}{\hat{\beta}_3} = -45.091$, which is statistically significant at the 1% level. 

+ This means that nonblack-female students received lower SAT scores by $45.091$ points on average compared to nonblack-male students, ceteris paribus. 

---

<span style="color:blue">
(iii) What is the estimated difference in SAT score between **nonblack-males** and **black males**? Test the null hypothesis that there is no difference between their scores, against the alternative that there is a difference.
</span>

<br>

**Black male students: (`black=1` and `female=0`) v.s.  Nonblack-males students: (`black=0` and `female=0`: base group)**

<br>

For black male students:

<br>

$$
\hat{sat} = \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 +  \color{blue}{\hat{\beta}_4}
$$

<br>

+ Hence, the estimated difference is $\color{blue}{\hat{\beta}_4} = −169.813$, which is statistically significant at the 1% level. 

+ This means that nonblack-female students received lower SAT scores by $169.813$ points on average compared to nonblack-male, ceteris paribus. 

---

<span style="color:blue">
(iv) What is the estimated difference in SAT score between **black-females** and **nonblack-females**? What would you need to do to test whether the difference is statistically significant?
</span>

**Black-female students: (`black=1` and `female=1`) v.s.  Nonblack-female students: (`black=0` and `female=1`: base group)**

For black-female students:

$$
\hat{sat} = \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 + \color{blue}{\hat{\beta}_3 +  \hat{\beta}_4 + \hat{\beta}_5}
$$

For nonblack-male students:

$$
\hat{sat} = \hat{\beta}_0 + \hat{\beta}_1 hsize + \hat{\beta}_2 hsize^2 + \color{blue}{\hat{\beta}_3} 
$$

<br>

+ Hence, the estimated difference is $(\color{blue}{\hat{\beta}_3 +  \hat{\beta}_4 + \hat{\beta}_5}) - \color{blue}{\hat{\beta}_3} = \color{blue}{\hat{\beta}_4 + \hat{\beta}_5} = -169.813 + 62.306 = -107.507$. 

+ To test weather the estimated difference between black-female students and nonblack-female students ( $\color{blue}{\hat{\beta}_4 + \hat{\beta}_5}$ ) is statistically significantly different from 0, we need to:

+ .content-box-green[**Approach 1**]: Conduct **t-test** under the following null and alternative hypothesis:

\begin{align}
H_0 &: \beta_4 + \beta_5 = 0 \\
H_1 &: \beta_4 + \beta_5 \neq 0
\end{align}

In this case, t-static would be 
$$
t = \frac{\beta_4 + \beta_5}{se(\beta_4 + \beta_5)}
$$

  * .content-box-green[**Approach 2**]: Chose one of black-female students and nonblack-female students to be the base group and re-estimate the new regression model. 

---

# Taking this one step further

+ In this example, we categorized four groups (nonblack-male, nonblack-female, black-male, black-female) based on gender (`black`) and race (`female`).

+ Instead of expressing these four groups with the combinations of `black` and `female`, why don't we use different dummy variables for each category?
  * For example, keep using nonblack-male students as base group, create new three dummy variables `nonblack_female`, `black_male`, and `black_female`, and use these dummy variables.

<br>

.content-box-blue[**Model(1) Use interactions involving dummy variables**]

$$
sat = \beta_0 + \beta_1 hsize + \beta_2 hsize^2 +\beta_3 female +  \beta_4 black + \beta_5 female \times black
$$

.content-box-blue[**Modle(2) Use dummy variables for each category**]

$$
sat = \beta_0 + \beta_1 hsize + \beta_2 hsize^2 +\beta_3 nonblack\\_female +  \beta_4 black\\_male + \beta_5 black\\_female
$$

---

.left5[

.content-box-blue[**Previous Results: Model(1) Use interactions involving dummy variables**]

+ Compared to white-male students, on average, SAT scores of :
  * nonblack-female students were lower by 45.091 points
  * balck-male students were lower by 169.813 points
  * black-female students were lower by 152.598 points

]

.right5[

.content-box-blue[**Results of Modle(2) Use dummy variables for each category**]


```{r, echo=F}
gpa_data <- read.dta13("./Data/Assignment_3/gpa2.dta") %>%
  data.table() %>%
  .[, hsize_2 := hsize^2] %>%
  .[, male := 1 - female] %>%
  .[, white := 1 - black] %>%
  .[, black_female := black * female] %>%
  .[, nonblack_female := white * female] %>%
  .[, balck_male :=  black*male]

# reg_gpa <- feols(sat ~ hsize + hsize_2 + female + black + I(female*black), data = gpa_data)

reg_gpa_1 <- feols(sat ~ hsize + hsize_2  + nonblack_female + balck_male + black_female, data = gpa_data)

msummary(
  reg_gpa_1,
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```
]

---

.left5[

.content-box-blue[**Previous Results: Model(1) Use interactions involving dummy variables**]

+ Compared to white-male students, on average, SAT scores of :
  * nonblack-female students were lower by 45.091 points
  * balck-male students were lower by 169.813 points
  * black-female students were lower by 152.598 points

]


.right5[

.content-box-blue[**Results of Modle(2) Use dummy variables for each category**]


```{r, echo=F}
gpa_data <- read.dta13("./Data/Assignment_3/gpa2.dta") %>%
  data.table() %>%
  .[, hsize_2 := hsize^2] %>%
  .[, male := 1 - female] %>%
  .[, white := 1 - black] %>%
  .[, black_female := black * female] %>%
  .[, nonblack_female := white * female] %>%
  .[, balck_male :=  black*male]

# reg_gpa <- feols(sat ~ hsize + hsize_2 + female + black + I(female*black), data = gpa_data)

reg_gpa_1 <- feols(sat ~ hsize + hsize_2  + nonblack_female + balck_male + black_female, data = gpa_data)

msummary(
  reg_gpa_1,
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```

<br>

.content-box-red[**The regression results from the Model(1) and Model(2) are the same**]
]



---


## So what's the difference between Model(1) and Model(2)?

.content-box-blue[**Model(1) Use interactions involving dummy variables**]

$$
sat = \beta_0 + \beta_1 hsize + \beta_2 hsize^2 +\beta_3 female +  \beta_4 black + \beta_5 female \times black
$$

.content-box-blue[**Model(2) Use dummy variables for each category**]

$$
sat = \beta_0 + \beta_1 hsize + \beta_2 hsize^2 +\beta_3 nonblack\\_female +  \beta_4 black\\_male + \beta_5 black\\_female
$$

<br>

+ Both Model(1) and Modle(2) can estimate the average differences in SAT scores across all gender-race combinations 
+ In addition, Model(1) tells whether the impact of gender depends on race
  *  Recall the role of an interaction term:
    - By interacting `female` and `black`, we allow the impact of gender depend on race
    - The coefficient of `female`-`back` interaction term is statistically significant, meaning that **the impact of gender does depend on race**.
  * The results of Model(2) alone cannot tell this information, and we need to conduct another test separately. (Ex, test to check whether the impact of `nonblack_female` is statistically different from `nonblack_male`)


.content-box-greenp[**Conclusion**]
+ In addition to estimating the the average difference in SAT scores across all gender-race combinations, Model (1) tells more information about the interaction effect between gender and race. 

---

# 2.2 Problem 6 :Quick review for the last review session

<span style="color:blue"> 
To test the effectiveness of a job training program on the subsequent wages of workers, we specify the model
</span>

$$
\begin{align}
    log(wage) = \beta_0 + \beta_1 train + \beta_2 educ + \beta_3 exper + u
\end{align}
$$

<span style="color:blue"> 
where $train$ is a binary variable equal to unity if a worker participated in the program. Think of the error term $u$ as containing unobserved worker ability.** If less able workers have a greater chance of being selected for the program**, and you use an OLS analysis, what can you say about the likely bias in the OLS estimator of $\beta_1$?
</span>

<br>

.content-box-green[**Theme: What is the direction of bias due to the omission of an importatn variable?**]

<br>

.content-box-blue[**Naive Model**]

$$
log(wage) = \beta_0 + \beta_1 train + \beta_2 educ + \beta_3 exper + u \quad (u = \beta_4 ability + v)
$$

<br>

+ Recall how the direction of bias can be approximated.
  * $ability$ is <span style="color:blue">negatively</span> correlated with $train$ (Cor(train, ability) < 0)
  * Generally, $ability$ should have a <span style="color:red"> positive </span>impact on $log(wage)$

+ Hence, $\hat{\beta}_1$ is likely to have a downward bias. (<span style="color:blue">negative</span> $\times$ <span style="color:red"> positive </span> = <span style="color:blue">negative</span>)

---

# 3.1 Problem C1 (Chapter 6)

<span style="color:blue"> 
Use the data in **KIELMC.dta**, only for the year 1981, to answer the following questions. The data are for houses that sold during 1981 in North Andover, Massachusetts; 1981 was the year construction began on a local garbage incinerator.
</span>

<br>

.content-box-green[**Main variables**]
+ `price`: housing price in dollars 
+ `dist`: distance from the house to the incinerator measured in feet

<br>

.content-box-green[**Objective**]
+ Estimate the impact of the presence of the incinerator on housing prices

<br>

.content-box-green[**Data**]
```{r}
# === Preparation === #
#--- load the data ---#
data <- read.dta13("./Data/Assignment_3/KIELMC.dta")
#--- subset the data to 1981 ---#
data_81 <- filter(data, year == 1981)
```


```{r, echo=F, eval=F}
ggplot(data_81, aes(x=dist, y=price))+
  geom_point() +
  geom_smooth()

ggplot(data_81, aes(x=dist, y=lprice))+
  geom_point() +
  geom_smooth()

ggplot(data_81, aes(x=ldist, y=price))+
  geom_point() +
  geom_smooth()

ggplot(data_81, aes(x=ldist, y=lprice))+
  geom_point() +
  geom_smooth()
```


---

<span style="color:blue"> 
(i) Run the following simple regression model:
</span>
$$
log(price) = \beta_0 + \beta_1 log(dist) + u
$$


The below is the regression result:

```{r, echo=F}
#--- OLS ---#
reg_c1_6_1 <- feols(log(price) ~ log(dist), data = data_81)

#--- msummary ---#
msummary(
  reg_c1_6_1,
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```

<br>

.content-box-green[**Interpretation**]
+ A one percent increase in distance from the incenarator will increase the housing prices by 0.365%, *ceteris paribus*.


---

<span style="color:blue"> 
(ii) Let's add more variables. Now the model is:
</span>

$$
log(price) = \beta_0 + \beta_1 log(dist) + \beta_2 log(intst) + \beta_3 log(area) + \beta_4 log(land) + \beta_5 rooms + \beta_6 baths + \beta_7 age + u
$$

.right5[
.content-box-green[**Interpretation**]
+ Now, the incinerator does not have any statistically significant impact on house price.

.content-box-green[**why**]
+ Omitted variable bias:
+ In the previous naive model, the impact of the incinerator might have been overestimated. 
]


.left5[
```{r c1_6_2, echo = F}
#--- OLS ---#
reg_c1_6_2 <- feols(log(price) ~ log(dist) + log(intst) + log(area) +
  log(land) + rooms + baths + age, data = data_81)

#--- msummary ---#
msummary(
  reg_c1_6_2,
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```
]

---

# Correlation matrix

+ The below shows that correlation between `dist` and newly added variables

```{r}
round(cor(select(data_81, dist, intst, area, land, rooms, baths, age)), digits = 2)
```

+ `dist` is highly correlated with `intst` (distance from the home to the interstate)

---

<span style="color:blue">
(iii) Add $[log(intst)]^2$ to the model from part (ii). Now what happens? 
</span>

.right5[
.content-box-green[**Interpretation**]
+ $(log(intst))^2$ is statistically significant
+ The incinerator have statistically significant impact on house price.

.content-box-green[**why?**]
+ We confirmed that `dist` and `intst` are highly correlated (Cor(`dist` and `intst`) = 0.89)
  * In this case, if OLS estimator of `intst` is biased then the OLS estimator of `dist` would be biased as well.

<br>

+ In the previous model:
  *  `log(intst)` alone was not enough to capture the  impact of `intst`　on `price` and the remaining part (`(log(intst))^2`) was left in the error term, which caused endogenous problem for the OLS estimator of `log(intst)` .
  * the coefficient on `log(intst)` is biased -> the coefficient on  `log(dist)` is biased as well. 
]


.left5[

```{r c1_6_3, echo = F}
#--- OLS ---#
reg_c1_6_3 <- feols(log(price) ~ log(dist) + log(intst) +  I(log(intst)*log(intst)) + log(area) + log(land) + rooms + baths + age, data = data_81)

#--- msummary ---#
msummary(
  reg_c1_6_3,
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```
]

---

<span style="color:blue">
(iv) Is the square of `log(dist)` significant when you add it to the model from part (iii)?
</span>


.right5[
```{r c1_6_4, echo=F}
#--- OLS ---#
reg_c1_6_4 <- feols(log(price) ~ log(dist) + I(log(dist)*log(dist)) + log(intst) +  I(log(intst)*log(intst)) + log(area) + log(land) + rooms + baths + age, data = data_81)

#--- msummary ---#
msummary(
  reg_c1_6_4,
  stars = TRUE,
  gof_omit = "IC|Log|Adj|F|Pseudo|Within"
)
```

]


.left5[
+ No, it is not.
  *  `log(dist)` is enough to capture all the impact of `dist` after we controlled other factors. 
]

---

## Taking this one step further

Recall the model in (iii):

\begin{align}
log(price) = \beta_0 &+ \beta_1 log(dist) + \beta_2 log(intst) + \beta_3 (log(intst))^2 +  \beta_4 log(area) \\
&+ \beta_5 log(land) + \beta_6 rooms + \beta_7 baths + \beta_8 age + u
\end{align}



+ We saw including `(log(intst))^2` dramatically changed the impact of the incinerator (`dist`)

<br>


+ **Let's think about the implication about the terms $\beta_2 log(intst) + \beta_3 (log(intst))^2$ a little bit more**

<br>

+ How can we interpret this?




---

## A Quick overview of logarithmic functional forms

.content-box-green[**Log-liner functional form: Y in log form**]

$$
log(Y) = \beta_0 + \beta_1 X + u
$$

<br>

.content-box-green[**<span style="color:red">Log-log functional form: Both X and Y are in log form</span>**]
+ it is also called constant elasticity model
$$
log(Y) = \beta_0 + \beta_1 log(X) + u
$$

<br>

.content-box-green[**Liner-log functional form: X in log form**]

$$
Y = \beta_0 + \beta_1 log(X) + u
$$



---


## .content-box-green[**Log-log functional form (aka Constant elasticity model)**]

<br>

.content-box-blue[**Model:** ]

$$
log(Y) = \beta_0 + \beta_1 log(X) + u \quad \mbox{where, } X, Y > 0
$$

<br>

.content-box-blue[**Interpretation:** ]
+ $\beta_1$ is the elasticity of Y with respect to X: 

\begin{align}
\beta_1 = \frac{dlog(Y)}{dlog(X)} &\approx \frac{\Delta Y/Y}{\Delta X/X} = \frac{\%\Delta Y}{\%\Delta X} \\
\newline
\Longrightarrow \%\Delta Y &\approx \beta_1 \%\Delta X
\end{align}



+ 1% increase in X increases Y by $\beta_1$% (constant elasticity)
+ or, Y always responds to 1% change in X by $\beta_1$%. 

<br>

.content-box-red[**Question:**]
+ How does the interpretation would be changed if we added a quadratic term of $log(X)$?

---

## .content-box-green[**How does the interpretation would be changed if we added a quadratic term of $log(X)$?**]

.content-box-blue[**Model:** ]

$$
log(Y) = \beta_0 + \beta_1 log(X) + \beta_2 (log(X))^2 + u \quad \mbox{where, } X, Y > 0
$$

<br>

+ log-log functional in the previous slide assumes a **constant** elasticity of Y with respect to X
+ However, assuming a constant elasticity might not be appropriate for some cases.
  * e.g., the response of Y to 1% change in X might change depending on the level of X. 

+ By using a quadratic along with logarithms, we allow a **nonconsistant** elasticity between Y and X
  * Remember a quadratic term of X allows decreasing or increasing marginal impact of X on Y


<br>

.content-box-blue[**Interpretation:**]
+ Now the elasticity of Y with respect to X depends on $log(X)$:

\begin{align}
\frac{dlog(Y)}{dlog(X)} &=  \beta_1 + 2\beta_2log(X) \\
\newline
\Longrightarrow \frac{\%\Delta Y}{\%\Delta X} &= \beta_1 + 2\beta_2log(X) \\
\newline
\Longrightarrow \%\Delta Y &= [\beta_1 + 2\beta_2log(X)]\%\Delta X 
\end{align}



```{r, echo=F, eval=F}
# # log(Y) &= 1 + 2 \times log(X)
# # \Longrightarrow Y &= eX^2
# data <- 
#   data.table(
#     x = seq(1,10, by=0.01)
#   ) %>%
#   .[,Model1 := exp(1)*x^2] %>%
#   .[,Model2 := exp(1)*x^(2-0.1*log(x))] %>%
#   melt(., id.vars = "x", measure.vars = c("Model1", "Model2"))  

# ggplot(data)+
#   geom_line(aes(x=x, y = value, color=variable))

# ggplot(data[variable=="Model2"])+
#   geom_line(aes(x=x, y = value))
```


---

## Going back to problem (iii)

The estimated model is:

$$
log(price) = −3.318 + 0.185 log(dist) + 2.073 log(intst) −0.119 (log(intst))^2 + ... 
$$ 

<br>

.content-box-blue[**Interpretation of the impact of `intst`:**]

+ The coefficient of $(log(intst))^2$ is <span style="color:blue">negative</span> and it is statistically significant, meaning that the elasticity of $price$ with respect to $intst$ varies depending on $log(intst)$ and <span style="color:blue">the elasticity diminishes as $intst$ increases</span>. 

<br>

+ 1% increase in $intst$ changes $house$ by $2.073 − 2 \cdot 0.119 log(intst)$%.

<br>

+ For example: Given $intst=2000$, it is expected that 1% increase in $intst$ changes $house$ by $2.073 − 2 \cdot 0.119 log(2000) = 0.2639852$% on average, ceteris paribus.


---



# Problem C13 (Chapter 8): Standard Error Estimation

<span style="color:blue"> 
(i) and (ii) Estimate the model
$$
children = \beta_0 + \beta_1 age + \beta_2 age^2 + \beta_3 educ + \beta_4 electric + \beta_5 urban + u
$$
and report the usual and heteroskedasticity-robust standard errors. Would you say the heteroskedasticity you found is practically important?
</span>

<br>

```{r c13_8, echo=F}
#--- load the data ---#
data_c13_8 <- read.dta13("./Data/Assignment_3/FERTIL2.dta") %>%
  data.table()
```


```{r, eval=F}
ls_res <- 
  list(
    "Default" = 
      feols(
        children ~ age + I(age*age) + educ + electric + urban,
        data = data_c13_8),
    "Hetero" = 
      feols(
        children ~ age + I(age*age) + educ + electric + urban,
        vcov = "hetero",
        data = data_c13_8)
  )

msummary(
  ls_res,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
  )
```

---

## Results

```{r, echo=F, eval=F}
data_c13_8[,.N, by=yearborn] %>% .[order(yearborn)]

hist(data_c13_8$yearborn)

data_c13_8[,.N, by=yearborn_class] %>% .[order(yearborn_class)]

ggplot()+
   geom_boxplot(data = data_c13_8, aes(x=factor(religion), y= educ))
```

```{r, echo=F}
data_c13_8 <- 
  data_c13_8 %>%
  mutate(
    religion = case_when(
      spirit == 1 ~ 1,
      protest == 1 ~ 2,
      catholic == 1 ~ 3,
      TRUE ~ 4,
    ),
    yearfm_group = case_when(
      yearfm < 70 ~ 1,
      yearfm >= 70 & yearfm < 80 ~ 2,
      yearfm >= 80  ~ 3
    )
  )

ls_res <- 
  list(
    "Default" = 
      feols(
        children ~ age + I(age*age) + educ + electric + urban,
        data = data_c13_8),
    "Hetero" = 
      feols(
        children ~ age + I(age*age) + educ + electric + urban,
        se = "hetero",
        data = data_c13_8)
    # "Cluster" = 
    #   feols(
    #     children ~ age + I(age*age) + educ + electric + urban,
    #     se = "cluster",
    #     cluster  =  ~religion,
    #     data = data_c13_8)
  )

msummary(
  ls_res,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
  )
```

.content-box-green[**Conclusion**]
+ Given how similar usual and robust standard errors are, I would say heteroskedasticity I found is not practically important.


```{r, echo=F, eval=F}
feols(
  children ~ age + I(age*age) + educ + electric + urban + spirit + protest +catholic,
  data = data_c13_8) %>%
  msummary(
  .,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
  )
```

---

# Data management

<span style="color:blue"> 
Reshape fake datasets (**corn_price_long.rds** and **corn_prod_wide.rds**) and merge them into a single dataset that is ready for statistical analysis.
</span>

.content-box-green[**corn_price_long.rds**]
+ Data on corn price by county and month for 2015

```{r, echo=F}
corn_price_long <- readRDS("./Data/Assignment_3/corn_price_long.rds")
print(head(corn_price_long))
```
]


.content-box-green[**corn_prod_wide.rds**]
+ Data on corn production by county (made-up) and month for 2015

```{r, echo=F}
corn_prod_wide <- readRDS("./Data/Assignment_3/corn_prod_wide.rds")
print(head(corn_prod_wide))
```

---

<span style="color:blue"> 
(1) Convert **corn_prod_wide.rds** into a long format in which each row represents the production level for a specific county-year combination. 
</span>

+ Step1: Write down the desirable table you want to get.(Recommend!)
+ Step2: 

```{r}
corn_prod_long <-
  corn_prod_wide %>%
  pivot_longer(
    # - the name of the columns to pivot into longer format - #
    cols = - county_code, #or you can use !county_code
    # - what the name of cols represents - #
    names_to = "month", 
    # - what the values stored in cols represents - #
    values_to = "production"
  )
```

Or you can do like this:

```{r, eval=F}
# === Manually select cols === #
corn_prod_long <-
  corn_prod_wide %>%
  pivot_longer(
    cols = c(as.character(1:12)), 
    names_to = "month", 
    values_to = "production"
  )
```

The final output is:
```{r, echo=F}
print(head(corn_prod_long))
```

---
<span style="color:blue"> 
Merge **corn_prod_long** and **corn_price_long** together. Would the single key $couty\_code$ be sufficient to merge them? Why or why not?
</span>

<br>

+ The key variable (`couty_code`) alone is not enough, and `month` is also necessary. 
+ The key variable should be the same data type between the two data. So, let's check. 

<br>

.content-box-green[**Check `county_code`**]

```{r}
class(corn_price_long$county_code)
class(corn_prod_long$county_code)
```

<br>

.content-box-green[**Check `month`**]

```{r}
class(corn_price_long$month)
class(corn_prod_long$month)
```

---

+ We need to fix `month`: either convert `corn_price_long$month`


```{r}
# === Convert data type of month in corn_prod_long to integer=== #
corn_prod_long$month <- as.integer(corn_prod_long$month)

# === Merge === #
corn_price_prod <- left_join(corn_prod_long, corn_price_long, by = c("county_code", "month"))
```

The final output is:
```{r, echo=F}
print(head(corn_price_prod))
```

---

## Let's convert long to wide
+ Using Convert **corn_price_long** data to wide format 

<br>
step1: write down what it looks like
step2:


```{r}
corn_price_long %>%
pivot_wider(
  # - a list of column tha uniquely identifies each observation - #
  id_cols = county_code, 
  # - new column names - #
  names_from = month, 
  # - values of the new columns -#
  values_from = price
)
```

```{r, echo=F}
corn_price_long %>%
pivot_wider(
  # - a list of column tha uniquely identifies each observation - #
  id_cols = county_code, 
  # - new column names - #
  names_from = month, 
  # - values of the new columns -#
  values_from = price
) %>%
print()
```

---

# References
+ Wooldridge, Jeffrey M. Introductory Econometrics: A Modern Approach. 4th ed., international student ed. [Mason (OH)]: South-Western, Cengage Learning, 2009.

