---
title: "Review Assignment 2"
author: "Shunkei Kakimoto, University of Nebraska Lincoln"
output:
  xaringan::moon_reader:
    # css: [default, metropolis, metropolis-fonts] 
    css: xaringan-themer_large.css 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: middle

```{r, child = '../setup.Rmd'}
```

```{r, include = F}
opts_knit$set(root.dir = "~/Dropbox/R-project/Applied_Econometrics_TA")

#--- load packages ---#
# suppressMessages(library(here))

# /*---- Data Wrangling ----*/
library(data.table)
library(tidyverse)

# --- regression --- #
# library(here)
library(fixest)
library(readstata13)
library(readr)

# /*---- Visualization ----*/
library(RColorBrewer)
library(patchwork)
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(viridis)
library(grid)
library(gridExtra)
library(GGally)

# /*---- Model Summary ----*/
library(stats)
library(modelsummary)
library(flextable)
library(officer)
library(officedown)
library(gt)
```


# Main Topic

## Omitted Variable bias and Multicollinearity
  * What is the problem?
  * How do they affect the properties of OLS estimator?

<br>

## Statistical Hypothesis Testing
  * Why is it necessary?
  * How does it work? 

## Prblems
+ Problem C6
+ Problem C8 and C9

---
# Problem C6: Omitted Variable Bias

Suppose that you are interested in estimating the causal impact of education on salary, and suppose that:

.content-box-blue[**True Model**]
$$log(wage) = \beta_0 + \beta_1 educ + \beta_2 IQ + u$$

If you correctly identify this model, then you'll get:
$$log(wage) = \hat{\beta_0} + \hat{\beta_1} educ + \hat{\beta_2} IQ$$

<br>

Now, suppose that you accidentally omitted $IQ$, and estimated the following model:

.content-box-blue[**Naive Model**]
$$log(wage) = \beta_0 + \beta_1 educ + v \quad (v = \beta_2 IQ + u)$$ 

Then you'll get:
$$log(wage) = \tilde{\beta_0} + \tilde{\beta_1} educ$$

Since $E[v|educ] = E[\beta_2 IQ + u|educ] \neq 0$, $\tilde{\beta_1}$ is biased. 

<br>

.content-box-red[**What is the direction of bias? upward or downward?**]

---
.content-box-red[**What is the direction of bias? upward or downward?**]

There is a simple relationship between $\hat{\beta_1}$ and $\tilde{\beta_1}$:
$$\tilde{\beta}_1=\hat{\beta}_1 + \hat{\beta}_2\tilde{\sigma}_1$$

,where ${\sigma}_1$ is the slope coefficient from the simple regression $IQ = \tilde{\sigma_0} + \tilde{\sigma_1} educ$

<br>

In other words, the bias in $\tilde{\beta}_1$ is described as:
$$Bias(\tilde{\beta}_1) = E[\tilde{\beta}_1] - \beta_1 = \beta_2 \tilde{\sigma}_1$$

.content-box-blue[**Implication**]
+ The direction and size of the bias in $\tilde{\beta}_1$ depend on the impact of the omitted variable on the dependent variable $(\beta_2)$, the correlation between the omitted variable and the key variable $(\tilde{\sigma}_1)$

<br>

.content-box-blue[**What do you think about the direction of bias in $\tilde{\beta_1}$ in our example?**]
+ The sign of $\beta_2$?
  * It is likely that $IQ$ affects $salary$ <span style="color:red">positively</span>
+ The sign of $\tilde{\sigma}_1$?  
  * It is likely that $IQ$ and $educ$ are <span style="color:red">positively</span> correlated

Hence, $\tilde{\beta_1}$ should have <span style="color:red">upward bias</span>: $E[\tilde{\beta_1}] > \beta_1$.  
+ Actually it is. We confirmed in problem (v)


---



# Problem C8 and C9

## Data
+ Zip code-level data on prices for various items at fast-food restaurant
```{r}
library(readstata13)
# --- Load the Data--- #
discrim_dt <- 
  read.dta13("./Data/Assignment_2/discrim.dta") %>%
  na.omit()
```

## The research question:
+ To know whether fast-food restaurants charge higher proces in areas with a larger concentration of Black People
+ Specifically, we want to know whether proportion of Black People (`prpblck`) living in ZIP code-level region affects average price soda (`psoda`) in that region, which we call "discrimination effect".

<br>

## Variable
+ $prpblck$: proportion of Black People
+ $income$: median income (in $)
+ $lincome$: logged $income$
+ $psoda$: average price of soda (in $)
+ $prppov$: proportion of people in poverty
+ $hseval$: median value of owner-occupied housing units (in $)
+ $lhseval$: logged $hseval$

---

# Example of a summary statistical table
+ This does not relevant to the problem but for your final paper.

```{r}
library(modelsummary)

datasummary(
  psoda + prpblck + income + prppov + hseval ~ Mean + SD + Min + Max,
  data = discrim_dt
)
```

As another option is

```{r, eval = F}
library(stargazer)
# set {r, results="asis"} for html output
discrim_dt %>%
  select(psoda, prpblck, income, prppov, hseval) %>%
  stargazer(data=., iqr=FALSE, type = 'html')

```

---

.content-box-green[**NOTE:** I changed the order of problems for to illustrate what your final project looks like.]

<span style="color:blue"> 
Let's start with simple model. Run a simple regression estimate from $psoda$ on $prpblck$. (Problem (iii))
</span>

```{r}
# === simple regression === #
discrim_reg_prob2 <- feols(psoda ~ prpblck, data = discrim_dt)
msummary(
  discrim_reg_prob2,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

+ Do you think that there is any other variables in the error term that could affect both  `prpblck` and `psoda`?
  * For example, median income (`income`).

---

<span style="color:blue"> 
Let's include `income` in the regression model. How does the discrimination effect change? (Problem (ii))
</span>


```{r}
discrim_reg1 <- 
  feols(
    psoda ~ prpblck + income, 
    data = discrim_dt)
```

```{r  echo = F}
msummary(
  discrim_reg1, 
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```


The discrimination effect becomes greater. 
+ .content-box-green[**Why?**] 
  * Since `income` is omitted in the previous model, the OLS estimates of `prpblck` in the simple regression model might be biased. 
    - `income` and `prpblck` are likely to be correlated <span style="color:blue">negatively</span>, and `income` is likely to affect `psoda` <span style="color:red">positively</span>. So if `income` is omitted, the discrimination effect is <span style="color:blue">underestimated</span>. 

---

<span style="color:blue"> 
(iv) A model with a constant price elasticity with respect to income may be more appropriate. Report estimates of the model:
$$log(psoda)=\beta_0+\beta_1 prpblck + \beta_2 log(income) +u $$
If $prpblck$ increases by .20 (20 percentage points), what is the estimated percentage change in $psoda$?
</span>

.content-box-green[**Idea**]
+ If the impact of `income` is not captured by correctly, a portion of the effect of `income` is still remaining in the error term. Then, the OLS estimate of the variable `prpblck` is biased (functional form missspecification problem). 
+ We modeled the relationship between price and income as electricity of `psoda` respect to `income`


```{r}
discrim_reg3 <- feols(log(psoda) ~ prpblck + lincome, data = discrim_dt)
```


```{r  echo = F}
msummary(
  discrim_reg3,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

.content-box-green[**Any changes?**]
  * The coefficient of `income` in the from of log became larger.
  * Also, the discrimination effect (the coefficient on $prpblck$) became larger

---

<span style="color:blue"> 
(v) Now add the variable $prppov$ to the regression in part (iv). What happens to $\beta_{prpblck}$?
</span>

Now, the model is 
$$log(psoda)=\beta_0+\beta_1 prpblck + \beta_2 log(income) + \beta_3 prppov +u $$

```{r}
discrim_reg4 <- feols(log(psoda) ~ prpblck+lincome+prppov, data = discrim_dt)
```

```{r  echo = F}
msummary(
  discrim_reg4,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

The discrimination effect now has a smaller impact on the price of soda.
+ .content-box-green[**A possible explanation**] 
  - proportion of people in poverty $(prppov)$ likely has a positive correlation on $prpblck$, and the impact on $psoda$ is positive.
  - So, if $prppov$ is omitted, the discrimination effect is likely to be overestimated. 

---

<span style="color:blue"> 
(vi) Find the correlation between $log(income)$ and $prppov$. Is it roughly what you expected?
</span>

```{r}
discrim_dt %>%
  select(prpblck, lincome, prppov) %>%
  cor()
```

+ $prppov$ is highly negatively correlated with $log(income)$, and it is highly correlated with $prpblck$ as well. 

---

<span style="color:blue"> 
(vii) Evaluate the following statement: "Because $log(income)$ and $prppov$ are so highly correlated, they have no business being in the same regression."
$$log(psoda)=\beta_0+\beta_1 prpblck + \beta_2 log(income) + \beta_3 prppov +u $$

</span>

.left5[
+ NO
+ Our interest is to measure the discrimination effect (which is described in the coefficient on $prpblck$) 

+ $prppov$ and $prpblck$ are higly correlated
+ $prppov$ has large impact on $psoda$ 

<br>

+ So, if you omitted $prppov$, the discrimination effect would be biased. 

]

.right5[

```{r  echo = F}
msummary(
  discrim_reg4,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

]

---

# Problem C9 (Statistical Hypothesis Testing)

## Quick review of statistical hypothesis testing

$$y_{i} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_k x_{ik} \mbox{ where  i = 1,2,..,n}$$

.content-box-green[**t-statstic and t-distribution**]
$$\frac{\hat{\beta_j}-\beta_j}{se(\hat{\beta_j})} \mbox{ ~ } t_{n-k-1}$$

+ $\beta_j$ is a true parameter (a specific constant $\alpha$), and $\hat{\beta_j}$ is an estimate of $\beta_j$

<br>

+ Intuitive understanding: t-statistic measures the difference between $\hat{\beta_j}$ and $\beta_j$ (<span style="color:red">How far is $\hat{\beta_j}$ from $\beta_j$?</span>).
+ First, we assume a specific constant (i.e., $\alpha$) for $\beta$ ((which is specified in <span style="color:blue">the null hypothesis</span>)

+ If $\hat{\beta_j}$ is indeed $\beta_j$, then the difference between $\hat{\beta_j}$ and $\beta_j$ measured by t-statics (i.e. <span style="color:blue"> </span>) should be small<br />
$\Leftrightarrow$ It should be very rare to observe the large absolute value of the t-value (i.e. |t-value|). <br />
$\Leftrightarrow$ The probability of observing a large |t-value| is very low. 
  * The probability to judge whether the |t-value| is large enough to say $\hat{\beta_j}$ and $\beta_j$ are different or not is called <span style="color:blue">statistical significance level</span> (e.g, 5%). 
  * If you set a statistical significance level, <span style="color:blue">critical value</span> is also  determined under the null hypothesis.
+ Observing larger |t-value| than |critical value| means that very unlikely thing occurs, so we reject the null. Otherwise, the null cannot be rejected.

+ The smallest significance level at which the null would be rejected is called <span style="color:blue">p-value</span> 

---
Since, we mostly interested in whether $x_j$ does have a partial effect on y or not, we use the null hypothesis $H_0 \mbox{: } \beta_j=0$.

.content-box-green[**For example**]
\begin{align}
H_0: \beta_j&=0 \
H_1: \beta_j& \neq 0
\end{align}


```{r, echo=FALSE, out.width = "60%"}
library(wooldridge)
data('wage1')

wage_reg <-lm(wage ~ educ + exper + tenure+ I(exper*exper), data = wage1)

u <- seq(-3, 3, length = 1000)
df <- 500 - 10
pdf <- dt(u, df = wage_reg$df.residual) # default is mean = 0,sd = 1
data <- data.table(x = u, y = pdf)

g_t <- ggplot() +
  geom_line(data=data,aes(y=y,x=x)) +
  geom_hline(yintercept=0) +
  xlab('') +
  ylab('') +
  theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()
    )

alpha <- 0.05

c_rej <- qt(1-alpha/2, df = wage_reg$df.residual)
rej_data_high <- data[u >= c_rej,]
rej_data_low <- data[u <= -c_rej,]

dist <- 
g_t +
  geom_ribbon(
    data = rej_data_high,
    aes(ymax = y, ymin = 0, x = x),
    fill = 'red', alpha = 0.4
    ) +
  geom_ribbon(
    data = rej_data_low,
    aes(ymax = y,ymin = 0,x = x),
    fill = 'red',alpha = 0.4
    ) +
  annotate('text',x = c_rej,y = -0.02,
    label = paste0("critical value: ", as.character(round(c_rej,digits = 2))),
    size = 4,
    family = 'Times'
    ) +
  annotate('text',x = -c_rej,y = -0.02,
    label = paste0("critical value: ", as.character(round(-c_rej,digits = 2))),
    size = 4,
    family = 'Times'
    ) +
  annotate('text',x = c_rej+0.2, y = 0.1,
    label = paste('area  = ',round(alpha/2,digits = 3),sep = ''),
    size = 4,
    family = 'Times'
    ) +
  annotate('text',x = -c_rej-0.2, y = 0.1,
    label = paste('area  = ',round(alpha/2,digits = 3),sep = ''),
    size = 4,
    family = 'Times'
    )+
  labs(title = "Example: 5% significance level")

dist
```

+ In this case, if the absolute value of the t-value is smaller than 1.96, the null cannot be rejected, meaning that "$\hat{\beta_j}$ is not statistically different from zero at the $5\%$ level"
+ Or,if you get p-value and the p-value is larger than your choice of significance level (here, $5\%$), then the null cannot be rejected.

---

# Problem C9
<span style="color:blue"> 
(i) Use OLS to estimate the model $log(psoda)=\beta_0+\beta_1 prpblck + \beta_2 log(income) + \beta_2 prppov + u$, and report the results in the usual form. Is $\beta_1$ statistically different from zero at the $5\%$ level against a two-sided alternative? What about at the $1\%$ level?
</span>


If you want to know p-value, then just evaluate the regression results of `feols`

```{r}
discrim_reg4
```

If you want to extract a p-value for a specific coefficient, for example

```{r}
coeftable(discrim_reg4)["prpblck", 4]
```

+ Since the $p$-value is less than $0.01$ and $0.05$, $\hat{\beta}_1$ is statistically significant at both at the $1\%$ and $5\%$ levels.


---

<span style="color:blue"> 
(ii) What is the correlation between $log(income)$ and $prppov$? Is each variable statistically significant at the $5\%$ level? Report the two-sided p-values.
</span>


```{r}
discrim_dt %>%
  select(lincome, prppov) %>%
  cor()
```

+ p-values for $log(income)$ and  $prppov$ are `r coeftable(discrim_reg4)["lincome", 4]` and `r coeftable(discrim_reg4)["lincome", 4]`, respectively. They are statistically significant at the $5\%$ significance. level

```{r, echo=F}
msummary(
  discrim_reg4,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

---

<span style="color:blue"> 
(iii) To the regression in part (i), add the variable log($hseval$) (logged median value of owner-occupied housing units (in $)). Interpret its coefficient and report the two-sided $p$-value for $H_0$: $\beta_{log(hseval)}=0$
</span>

Now the model is 
$$log(psoda)=\beta_0+\beta_1 prpblck + \beta_2 log(income) + \beta_3 prppov + \beta_4 log(hseval) + u$$

```{r}
discrim_reg5 <- feols(log(psoda) ~ prpblck+lincome+prppov+lhseval, data = discrim_dt)
```


```{r, echo=F}
msummary(
  discrim_reg5,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

.content-box-green[**Any changes?**]
+ the discrimination effect becomes to has more impact 
+ $log(hsevalue)$ is statistically significantly different from $0$ at 0.1%. 
+ Meanwhile, $log(income)$ and $prppve$ turned out to be not statistically significant, individually. 
  * .content-box-blue[**Why?**]

---

<span style="color:blue"> 
(iv) In the regression in part (iii), what happens to the individual statistical significance of $log(income)$ and $prppov$? Are these variables jointly significant? (Compute a p-value.) What do you make of your answers?
</span>

The correlation between $log(hseval)$, $log(income)$, and $prppov$ is 

```{r}
discrim_dt %>%
  select(lincome, prppov, lhseval) %>%
  cor()
``` 

+ $prppov$ is highly correlated with $prpblck$ and $log(income)$. Because of the multicollinearity problem, the standard error of $prpblck$ and $log(income)$ increased, resulting in insignificant statistical results for $prpblck$ and $log(income)$, individually.


---

Let's conduct F-test to see whether the impact of $log(income)$ and $prppov$ are jointly significant. 

```{r}
# === R package for F-test === #
library(car)
# === Note: you need to use lm() to use car package === #
temp <- lm(log(psoda) ~ prpblck+lincome+prppov+lhseval, data = discrim_dt)
# === F-test === #
linearHypothesis(temp, c("lincome=0", "prppov=0"))
```

```{r, echo=F}
linearHypothesis(temp, c("lincome=0", "prppov=0")) %>% print
```
.content-box-green[**Conclusion**]
+ The p-value is `r linearHypothesis(temp, c("lincome=0", "prppov=0"))[["Pr(>F)"]][2]`. They are jointly significant at the $10\%$ level.

---

<span style="color:blue"> 
(v) Given the results of the previous regressions, which one would you report as most reliable in determining whether the racial makeup of a zip code influences local fast-food prices? (**bias-variance trade-off**)
</span>

\begin{align}
\mbox{Model1: } \quad & psoda && = \hat{\beta_0}+\hat{\beta_1} prpblck \\
\mbox{Model2: } \quad & psoda && = \hat{\beta_0}+\hat{\beta_1} prpblck + \hat{\beta_2} income \\
\mbox{Model3: } \quad & log(psoda) && = \hat{\beta_0}+\hat{\beta_1} prpblck + \hat{\beta_2} log(income) \\
\mbox{Model4: } \quad & log(psoda) && = \hat{\beta_0}+\hat{\beta_1} prpblck + \hat{\beta_2} log(income) + \hat{\beta_3} prppov \\
\mbox{Model5: } \quad & log(psoda) && =\hat{\beta_0}+\hat{\beta_1} prpblck + \hat{\beta_2} log(income) + \hat{\beta_3} prppov + \hat{\beta_4} log(hseval)
\end{align}

+ Our main focus is on the estimation of the discrimination effect $(\hat{\beta}_{prpblck})$ correctly. 
  * In terms of unbiasedness, $(\hat{\beta}_{prpblck})$ in Model5 (including log(income), prpblck,and log(hseval)) is less biased compared to those in other models.
  * In terms of efficiency, it is not certain whether we should include $log(income)$ and $prpblck$ (because it is highly correlated with $log(income)$ and $prppov$, and they have some explanatory power) due to multicollinearity problem.  

.content-box-green[**In summary**.]
+ I would choose Model 5
  * (1) Omission of $log(income)$ and $prppov$ cause omitted variable bias to $(\hat{\beta}_{prpblck})$ which we want to estimate accurately 
  * (2) $prpblck$ is statistically significant even after including $log(income)$ and $prpblck$


```{r}
discrim_dt %>%
  select(prpblck, lincome, prppov, lhseval) %>%
  cor()
``` 


---

# Problem C5 (Chapter 4)

+ $salary$: annual salary
+ $years$: number of years in MLB
+ $gamesyr$: number of games played in a year
+ $bavg$: batting average
+ $hrunsyr$: home runs per year
+ $rbisyr$: runs batted in per year

## Data

```{r}
mlb_data <- read_csv('./Data/Assignment_2/MLB1.csv')
```


---

<span style="color:blue"> 
(i) First, estimate the following model:
$$
\widehat{log(salary)}=\beta_0+\beta_1 years + \beta_2 gamesyr + \beta_3 bavg+\beta_4 hrunsyr+\beta_5 rbisyr + v
$$
Now, drop the variable $rbisyr$ and then estimate. What happens to the statistical significance of $hrunsyr$? What about the size of the coefficient on $hrunsyr$? Provide explanations of why these changes in the statistical significance and the size of estimated coefficient on $hrunsyr$ happened?
</span>


.left5[
+ Include $rbisyr$

```{r, echo = F}
mlb_reg1 <- feols(log(salary)~years+gamesyr+bavg+hrunsyr+rbisyr,data=mlb_data)

msummary(
  mlb_reg1,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

]

.right5[
+ Omit $rbisyr$

```{r  echo = F}
mlb_reg2 <- feols(log(salary)~years+gamesyr+bavg+hrunsyr,data=mlb_data)

msummary(
  mlb_reg2,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

]

---

```{r}
mlb_data %>%
  select(hrunsyr,rbisyr) %>%
  cor()
```

+ The insignificance of $hrunsyr$ when $rbisyr$ is due to high correlation between the two variables.

---

<span style="color:blue"> 
(ii) Add the variables $runsyr$ (runs per year), $fldperc$ (fielding percentage), and $sbasesyr$ (stolen bases per year) to the model from part (i). Which of these factors are individually significant?
</span>

.left5[

+ Of the additional independent variables, only $runsyr$ is statistically significant.

+ Correlation?

]

.right5[

```{r  echo = F}
mlb_reg3 <- feols(log(salary)~years+gamesyr+bavg+hrunsyr+rbisyr+runsyr+fldperc+sbasesyr,data=mlb_data)

msummary(
  mlb_reg3,
  stars  =  TRUE, 
  gof_omit  =  "IC|Log|Adj|F|Pseudo|Within" 
)
```

]


---

<span style="color:blue"> 
(iii) In the model from part (ii), test the joint significance of $bavg$, $fldperc$, and $sbasesyr$.
</span>

```{r}
# === Note: you need to use lm() to use car package === #
temp <- lm(log(salary)~years+gamesyr+bavg+hrunsyr+rbisyr+runsyr+fldperc+sbasesyr,data=mlb_data)
# === F-test === #
linearHypothesis(temp, c("bavg=0", "fldperc=0", "sbasesyr=0"))
```

+ They are not statistically significant. Therefore, there would be little harm in dropping those variables. 

---

# References
+ Wooldridge, Jeffrey M. Introductory Econometrics: A Modern Approach. 4th ed., international student ed. [Mason (OH)]: South-Western, Cengage Learning, 2009.

